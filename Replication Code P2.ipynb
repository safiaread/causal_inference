{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7d2afd54",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Replication Project Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "96e55311",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "965cb645",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_stata(\"Downloads/AER20090377_FinalData.dta\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7abc4826",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "neighbor_data = pd.read_stata(\"Downloads/112465-V1/AER20090377_DataAndPrograms/AER20090377_NeighborData.dta\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fee97e1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set memory, processors, and other configurations (not directly equivalent in pandas)\n",
    "# These configurations are often not necessary in Python, as memory management and parallel processing are typically handled differently.\n",
    "\n",
    "# Load the data\n",
    "final_data = data.copy()\n",
    "\n",
    "# Data preparation\n",
    "# Keep years before 2004\n",
    "final_data = final_data[final_data['year'] < 2004]\n",
    "\n",
    "# Drop rows with invalid observations\n",
    "final_data = final_data[final_data['valid'] >= 9]\n",
    "\n",
    "# Keep only Jun-Aug months\n",
    "final_data = final_data[final_data['month'].isin([6, 7, 8])]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4bb9ad4a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fips</th>\n",
       "      <th>site_id</th>\n",
       "      <th>year</th>\n",
       "      <th>NumDays</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1001.0</td>\n",
       "      <td>3</td>\n",
       "      <td>1990.0</td>\n",
       "      <td>92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1003.0</td>\n",
       "      <td>10</td>\n",
       "      <td>2000.0</td>\n",
       "      <td>89</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1003.0</td>\n",
       "      <td>10</td>\n",
       "      <td>2001.0</td>\n",
       "      <td>91</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1003.0</td>\n",
       "      <td>10</td>\n",
       "      <td>2002.0</td>\n",
       "      <td>89</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1003.0</td>\n",
       "      <td>10</td>\n",
       "      <td>2003.0</td>\n",
       "      <td>88</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     fips  site_id    year  NumDays\n",
       "0  1001.0        3  1990.0       92\n",
       "1  1003.0       10  2000.0       89\n",
       "2  1003.0       10  2001.0       91\n",
       "3  1003.0       10  2002.0       89\n",
       "4  1003.0       10  2003.0       88"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_days_per_year = final_data.groupby(['fips', 'site_id', 'year']).size().reset_index(name='NumDays')\n",
    "num_days_per_year = num_days_per_year[num_days_per_year['NumDays'] >= 69]\n",
    "num_days_per_year.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "db9ea690",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep monitor-years with at least 75% of days with observations\n",
    "final_data = final_data.merge(num_days_per_year[['fips', 'site_id', 'year', 'NumDays']], on=['fips', 'site_id', 'year'], how='right')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d805cf7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort the DataFrame\n",
    "final_data.sort_values(by=['fips', 'site_id', 'year', 'ozone_max'], inplace=True)\n",
    "\n",
    "# Collapse to get the mean ozone_max by fips, site_id, and year\n",
    "temp_data = final_data.groupby(['fips', 'site_id', 'year'])['ozone_max'].mean().reset_index().copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d0ca707f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by 'fips' and 'site_id' and count the number of unique years\n",
    "final_data['NumYear'] = final_data.groupby(['fips', 'site_id'])['year'].transform('count')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b6dc4823",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     state_code  county_code  site_id  valid   epa_8hr  ozone_max   day  \\\n",
      "1             1            1        3   11.0  0.016714      0.020   2.0   \n",
      "108           1            3       10   12.0  0.018250      0.019  18.0   \n",
      "247           1            3       10   12.0  0.018750      0.021   7.0   \n",
      "295           1            3       10   12.0  0.017875      0.020  27.0   \n",
      "397           1            3       10   12.0  0.013000      0.015   7.0   \n",
      "\n",
      "     month    year       Date  ...  NumOffMax NumOffMin NumOff1Max NumOff1Min  \\\n",
      "1      6.0  1990.0 1990-06-02  ...        0.0       0.0        0.0        0.0   \n",
      "108    6.0  2000.0 2000-06-18  ...        0.0       1.0        0.0        1.0   \n",
      "247    8.0  2001.0 2001-08-07  ...        0.0       1.0        0.0        1.0   \n",
      "295    6.0  2002.0 2002-06-27  ...        0.0       1.0        0.0        1.0   \n",
      "397    7.0  2003.0 2003-07-07  ...        2.0       1.0        2.0        1.0   \n",
      "\n",
      "    NOtherStationprcp _merge urban  _mergeurb NumDays  NumYear  \n",
      "1                 NaN    3.0   3.0          3      92       92  \n",
      "108               NaN    3.0   2.0          3      89      357  \n",
      "247               NaN    3.0   2.0          3      91      357  \n",
      "295               NaN    3.0   2.0          3      89      357  \n",
      "397               NaN    3.0   2.0          3      88      357  \n",
      "\n",
      "[5 rows x 66 columns]\n",
      "fips     site_id  year  \n",
      "4013.0   13       1990.0    69\n",
      "37183.0  17       1994.0    69\n",
      "20173.0  10       1990.0    69\n",
      "4013.0   2004     1994.0    69\n",
      "9001.0   17       1992.0    69\n",
      "                            ..\n",
      "37109.0  4        2003.0    92\n",
      "                  2002.0    92\n",
      "                  2001.0    92\n",
      "                  1999.0    92\n",
      "1001.0   3        1990.0    92\n",
      "Length: 14394, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Drop duplicate rows based on 'fips', 'site_id', and 'year' columns\n",
    "final_data_no_duplicates = final_data.drop_duplicates(subset=['fips', 'site_id', 'year'])\n",
    "\n",
    "# Display the first few rows of the DataFrame without duplicates\n",
    "print(final_data_no_duplicates.head())\n",
    "\n",
    "counts = final_data.groupby(['fips', 'site_id', 'year']).size()\n",
    "\n",
    "# Filtering combinations with 15 or fewer occurrences\n",
    "filtered_counts = counts[counts <= 15]\n",
    "\n",
    "# Getting the total number of rows\n",
    "total_rows = filtered_counts.sum()\n",
    "\n",
    "sorted_counts = counts.sort_values()\n",
    "print(sorted_counts)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9c2ba252",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by 'fips' and 'site_id' and get unique 'year' values for each group\n",
    "year_table = final_data.groupby(['fips', 'site_id'])['year'].unique().reset_index()\n",
    "\n",
    "# Count the number of years in each row\n",
    "year_table['num_years'] = year_table['year'].apply(len)\n",
    "\n",
    "year_table.head()\n",
    "\n",
    "year_table_filtered = year_table[year_table['num_years'] == 15]\n",
    "\n",
    "\n",
    "filtered_final_data = final_data.merge(year_table_filtered[['fips', 'site_id']], on=['fips', 'site_id'], how='right')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "16f3e5f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_history_monitors = filtered_final_data.to_stata('FullHistoryMonitors.dta')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d9388a31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort both DataFrames by 'fips' and 'site_id'\n",
    "temp_data.sort_values(by=['fips', 'site_id'], inplace=True)\n",
    "filtered_final_data.sort_values(by=['fips', 'site_id'], inplace=True)\n",
    "\n",
    "# Merge the datasets on 'fips' and 'site_id'\n",
    "new_data = pd.merge(temp_data, filtered_final_data, on=['fips', 'site_id'], how='right')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "fcc7ae78",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0         0.017\n",
       "1         0.018\n",
       "2         0.019\n",
       "3         0.022\n",
       "4         0.028\n",
       "          ...  \n",
       "469851    0.083\n",
       "469852    0.083\n",
       "469853    0.084\n",
       "469854    0.088\n",
       "469855    0.105\n",
       "Name: ozone_max, Length: 469856, dtype: float32"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered_final_data['ozone_max']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8421202e",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"['year'] not in index\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m new_data\u001b[38;5;241m.\u001b[39msort_values(by\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfips\u001b[39m\u001b[38;5;124m'\u001b[39m, inplace\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Merge main_data with neighbor_data on 'fips'\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m neighbor_data \u001b[38;5;241m=\u001b[39m neighbor_data\u001b[38;5;241m.\u001b[39mmerge(new_data[[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfips\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msite_id\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124myear\u001b[39m\u001b[38;5;124m'\u001b[39m]], on\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfips\u001b[39m\u001b[38;5;124m'\u001b[39m], how\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minner\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/core/frame.py:3767\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3765\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_iterator(key):\n\u001b[1;32m   3766\u001b[0m         key \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(key)\n\u001b[0;32m-> 3767\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39m_get_indexer_strict(key, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcolumns\u001b[39m\u001b[38;5;124m\"\u001b[39m)[\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m   3769\u001b[0m \u001b[38;5;66;03m# take() does not accept boolean indexers\u001b[39;00m\n\u001b[1;32m   3770\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(indexer, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdtype\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mbool\u001b[39m:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/core/indexes/base.py:5877\u001b[0m, in \u001b[0;36mIndex._get_indexer_strict\u001b[0;34m(self, key, axis_name)\u001b[0m\n\u001b[1;32m   5874\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   5875\u001b[0m     keyarr, indexer, new_indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reindex_non_unique(keyarr)\n\u001b[0;32m-> 5877\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_raise_if_missing(keyarr, indexer, axis_name)\n\u001b[1;32m   5879\u001b[0m keyarr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtake(indexer)\n\u001b[1;32m   5880\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key, Index):\n\u001b[1;32m   5881\u001b[0m     \u001b[38;5;66;03m# GH 42790 - Preserve name from an Index\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/core/indexes/base.py:5941\u001b[0m, in \u001b[0;36mIndex._raise_if_missing\u001b[0;34m(self, key, indexer, axis_name)\u001b[0m\n\u001b[1;32m   5938\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNone of [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m] are in the [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00maxis_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m]\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   5940\u001b[0m not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(ensure_index(key)[missing_mask\u001b[38;5;241m.\u001b[39mnonzero()[\u001b[38;5;241m0\u001b[39m]]\u001b[38;5;241m.\u001b[39munique())\n\u001b[0;32m-> 5941\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnot_found\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m not in index\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mKeyError\u001b[0m: \"['year'] not in index\""
     ]
    }
   ],
   "source": [
    "# Sort main_data by 'fips'\n",
    "new_data.sort_values(by='fips', inplace=True)\n",
    "\n",
    "# Merge main_data with neighbor_data on 'fips'\n",
    "neighbor_data = neighbor_data.merge(new_data[['fips', 'site_id', 'year']], on=['fips'], how='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44f80e7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "neighbor_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f60f8230",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Turn off RFG when CARB is on\n",
    "merged_data.loc[merged_data['treat_CARB'] != 0, 'treat_rfg'] = 0\n",
    "\n",
    "temp_data = merged_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7de3f69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate interaction terms\n",
    "temp_data.loc[:, '_DMTempMax'] = temp_data['DOW'] * temp_data['TempMax']\n",
    "temp_data.loc[:, '_DmTempMin'] = temp_data['DOW'] * temp_data['TempMin']\n",
    "temp_data.loc[:, '_DrRain'] = temp_data['DOW'] * temp_data['Rain']\n",
    "temp_data.loc[:, '_DsSnow'] = temp_data['DOW'] * temp_data['Snow']\n",
    "\n",
    "# Generate 'DOY' (day of year)\n",
    "temp_data.loc[:, 'DOY'] = temp_data['Date'].dt.dayofyear\n",
    "\n",
    "#Temperature Polynomials\n",
    "temp_data.loc[:, 'TempMax1'] = temp_data['TempMax']\n",
    "temp_data.loc[:, 'TempMax2'] = temp_data['TempMax'] * temp_data['TempMax']\n",
    "temp_data.loc[:, 'TempMax3'] = temp_data['TempMax'] * temp_data['TempMax'] * temp_data['TempMax']\n",
    "temp_data.loc[:, 'TempMin1'] = temp_data['TempMin']\n",
    "temp_data.loc[:, 'TempMin2'] = temp_data['TempMin'] * temp_data['TempMin']\n",
    "temp_data.loc[:, 'TempMin3'] = temp_data['TempMin'] * temp_data['TempMin'] * temp_data['TempMin']\n",
    "temp_data.loc[:, 'TempMaxMin'] = temp_data['TempMin'] * temp_data['TempMax']\n",
    "temp_data.loc[:, 'Rain1'] = temp_data['Rain']\n",
    "temp_data.loc[:, 'Rain2'] = temp_data['Rain'] * temp_data['Rain']\n",
    "temp_data.loc[:, 'Snow1'] = temp_data['Snow']\n",
    "temp_data.loc[:, 'Snow2'] = temp_data['Snow'] * temp_data['Snow']\n",
    "temp_data.loc[:, 'RainTempMax'] = temp_data['Rain'] * temp_data['TempMax']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "390b652d",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_data = temp_data.sort_values(by=['fips', 'site_id', 'Date'])\n",
    "temp_data['TempMaxL1'] = temp_data.groupby(['fips', 'site_id'])['TempMax'].shift(1)\n",
    "temp_data.loc[temp_data.groupby(['fips', 'site_id'])['TempMaxL1'].head(1).index, 'TempMaxL1'] = np.nan\n",
    "temp_data['TempMinL1'] = temp_data.groupby(['fips', 'site_id'])['TempMin'].shift(1)\n",
    "temp_data.loc[temp_data.groupby(['fips', 'site_id'])['TempMinL1'].head(1).index, 'TempMinL1'] = np.nan\n",
    "temp_data['TempMaxMaxL1'] = temp_data['TempMax'] * temp_data['TempMaxL1']\n",
    "temp_data['TempMaxMinL1'] = temp_data['TempMax'] * temp_data['TempMinL1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a291c92a",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_index = temp_data.columns.get_loc('TempMax1')\n",
    "end_index = temp_data.columns.get_loc('TempMaxMinL1') + 1\n",
    "for var in temp_data.columns[start_index:end_index]:\n",
    "    temp_data['DOY' + var] = temp_data['DOY'] * temp_data[var]\n",
    "\n",
    "# Keep only Jun-Aug\n",
    "temp_data = temp_data[temp_data['month'].isin([6, 7, 8])]\n",
    "\n",
    "# Save the modified data\n",
    "temp_data.to_stata(file_path + \"temp.dta\", write_index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bcb83b4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Additional variables: income. Take logs of ozone\n",
    "\n",
    "income_data = pd.read_stata(\"Downloads/112465-V1/AER20090377_DataAndPrograms/AER20090377_IncomeData.dta\")\n",
    "\n",
    "income_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fa542aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8224d8e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_data.sort_values(by=['state_code', 'county_code', 'year'], inplace=True)\n",
    "\n",
    "# Merge temp_data with income_data on state_code, county_code, and year\n",
    "merged_data = pd.merge(temp_data, income_data, on=['state_code', 'county_code', 'year'], how='inner', suffixes=('_temp', '_income'))\n",
    "\n",
    "merged_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcd3dd28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop rows where ozone_max or epa_8hr is equal to 0\n",
    "merged_data = merged_data[(merged_data['ozone_max'] != 0) & (merged_data['epa_8hr'] != 0)]\n",
    "\n",
    "# Drop rows where any of the TempMax1-TempMax3 or TempMin1-TempMin3 variables is missing\n",
    "temp_max_vars = [f\"TempMax{i}\" for i in range(1, 4)]\n",
    "temp_min_vars = [f\"TempMin{i}\" for i in range(1, 4)]\n",
    "merged_data = merged_data.dropna(subset=temp_max_vars + temp_min_vars)\n",
    "\n",
    "# Drop rows where income is missing\n",
    "merged_data = merged_data.dropna(subset=['income'])\n",
    "\n",
    "merged_data['lozone_max'] = np.log10(merged_data['ozone_max'])\n",
    "merged_data['lozone_8hr'] = np.log10(merged_data['epa_8hr'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8ff1f1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create census region dummies and interactions. Create time trend and interactions\n",
    "\n",
    "merged_data['region'] = np.nan\n",
    "merged_data['division'] = np.nan\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4882d740",
   "metadata": {},
   "outputs": [],
   "source": [
    "state_region_mapping = {\n",
    "    1: 3, 2: 4, 4: 4, 5: 3, 6: 4, 8: 4, 9: 1, 10: 3, 11: 3, 12: 3,\n",
    "    13: 3, 15: 4, 16: 4, 17: 2, 18: 2, 19: 2, 20: 2, 21: 3, 22: 3, 23: 1,\n",
    "    24: 3, 25: 1, 26: 2, 27: 2, 28: 3, 29: 2, 30: 4, 31: 2, 32: 4, 33: 1,\n",
    "    34: 1, 35: 4, 36: 1, 37: 3, 38: 2, 39: 2, 40: 3, 41: 4, 42: 1, 44: 1,\n",
    "    45: 3, 46: 2, 47: 3, 48: 3, 49: 4, 50: 1, 51: 3, 53: 4, 54: 3, 55: 2, 56: 4\n",
    "}\n",
    "\n",
    "# Update the 'region' column based on the state_code\n",
    "merged_data.loc[merged_data['state_code'] == 1, 'region'] = 3\n",
    "merged_data.loc[merged_data['state_code'] == 2, 'region'] = 4\n",
    "# Repeat the above line for each state_code and corresponding region using the state_region_mapping dictionary\n",
    "\n",
    "# You can also use a loop to iterate over the state_region_mapping dictionary\n",
    "for state_code, region in state_region_mapping.items():\n",
    "    merged_data.loc[merged_data['state_code'] == state_code, 'region'] = region\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11d2b0ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_data.loc[:, '_RY'] = merged_data['year']*merged_data['region']\n",
    "merged_data.loc[:, '_RW'] = merged_data['DOW']*merged_data['region']\n",
    "merged_data.loc[:, '_RD'] = merged_data['DOY']*merged_data['region']\n",
    "merged_data['DateS'] = merged_data['Date'].dt.dayofyear / 365\n",
    "merged_data['DateS2'] = (merged_data['DateS'])**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dbe763f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the specified columns (commented out on repeat)\n",
    "merged_data.drop(columns=['RVPCty', 'RFGCty', 'CARBCty'], inplace=True)\n",
    "\n",
    "# Group by 'fips' and create new variables 'RVPCty' and 'RFGCty' within each group\n",
    "merged_data['RVPCty'] = merged_data.groupby('fips')['treat_rvpII'].transform('max')\n",
    "merged_data['RFGCty'] = merged_data.groupby('fips')['treat_rfg'].transform('max')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "953061f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_data.loc[:, 'RVPRFGCty'] = 0\n",
    "# Replace RVPRFGCty with 1 where both RFGCty and RVPCty are equal to 1\n",
    "merged_data.loc[(merged_data['RFGCty'] == 1) & (merged_data['RVPCty'] == 1), 'RVPRFGCty'] = 1\n",
    "\n",
    "# Replace RFGCty with 0 where RVPRFGCty is equal to 1\n",
    "merged_data.loc[merged_data['RVPRFGCty'] == 1, 'RFGCty'] = 0\n",
    "\n",
    "# Replace RVPCty with 0 where RVPRFGCty is equal to 1\n",
    "merged_data.loc[merged_data['RVPRFGCty'] == 1, 'RVPCty'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be0c8f81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming merged_data is a pandas DataFrame\n",
    "merged_data['CARBCty'] = merged_data.groupby('fips')['treat_CARB'].transform('max')\n",
    "merged_data.loc[:, 'CARBRFGCty'] = 0\n",
    "\n",
    "# Replace CARBRFGCty with 1 if either RFGCty or RVPRFGCty is 1 and CARBCty is 1\n",
    "merged_data['CARBRFGCty'] = ((merged_data['RFGCty'] == 1) | (merged_data['RVPRFGCty'] == 1)) & (merged_data['CARBCty'] == 1)\n",
    "\n",
    "# Replace RFGCty with 0 if CARBRFGCty is 1\n",
    "\n",
    "merged_data.loc[merged_data['CARBRFGCty'] == 1, 'RFGCty'] = 0\n",
    "\n",
    "merged_data.loc[merged_data['CARBRFGCty'] == 1, 'RVPRFGCty'] = 0\n",
    "\n",
    "merged_data.loc[merged_data['CARBRFGCty'] == 1, 'CARBCty'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "882c5c6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1, 5):\n",
    "    merged_data[f'TrendRVP{i}'] = 0\n",
    "    merged_data.loc[(merged_data['RVPCty'] == 1) & (merged_data['region'] == i), f'TrendRVP{i}'] = merged_data['DateS']\n",
    "for i in range(1, 5):\n",
    "    merged_data[f'TrendRFG{i}'] = 0\n",
    "    merged_data.loc[(merged_data['RFGCty'] == 1) & (merged_data['region'] == i), f'TrendRFG{i}'] = merged_data['DateS']\n",
    "for i in range(1, 5):\n",
    "    merged_data[f'TrendRVPRFG{i}'] = 0\n",
    "    merged_data.loc[(merged_data['RVPRFGCty'] == 1) & (merged_data['region'] == i), f'TrendRVPRFG{i}'] = merged_data['DateS']\n",
    "for i in range(1, 5):\n",
    "    merged_data[f'TrendCARB{i}'] = 0\n",
    "    merged_data.loc[(merged_data['CARBCty'] == 1) & (merged_data['region'] == i), f'TrendCARB{i}'] = merged_data['DateS']  \n",
    "for i in range(1, 5):\n",
    "    merged_data[f'TrendCARBRFG{i}'] = 0\n",
    "    merged_data.loc[(merged_data['CARBRFGCty'] == 1) & (merged_data['region'] == i), f'TrendCARBRFG{i}'] = merged_data['DateS']    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "046571a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1, 5):\n",
    "    merged_data[f'QTrendRVP{i}'] = 0\n",
    "    merged_data.loc[(merged_data['RVPCty'] == 1) & (merged_data['region'] == i), f'QTrendRVP{i}'] = merged_data['DateS']**2\n",
    "\n",
    "for i in range(1, 5):\n",
    "    merged_data[f'QTrendRFG{i}'] = 0\n",
    "    merged_data.loc[(merged_data['RFGCty'] == 1) & (merged_data['region'] == i), f'QTrendRFG{i}'] = merged_data['DateS']**2\n",
    "    \n",
    "for i in range(1, 5):\n",
    "    merged_data[f'QTrendRVPRFG{i}'] = 0\n",
    "    merged_data.loc[(merged_data['RVPRFGCty'] == 1) & (merged_data['region'] == i), f'QTrendRVPRFG{i}'] = merged_data['DateS']**2\n",
    "    \n",
    "for i in range(1, 5):\n",
    "    merged_data[f'QTrendCARB{i}'] = 0\n",
    "    merged_data.loc[(merged_data['CARBCty'] == 1) & (merged_data['region'] == i), f'QTrendCARB{i}'] = merged_data['DateS']**2\n",
    "    \n",
    "for i in range(1, 5):\n",
    "    merged_data[f'QTrendCARBRFG{i}'] = 0\n",
    "    merged_data.loc[(merged_data['CARBRFGCty'] == 1) & (merged_data['region'] == i), f'QTrendCARBRFG{i}'] = merged_data['DateS']**2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbcadf7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort the DataFrame by 'state_code' and 'year'\n",
    "merged_data.sort_values(by=['state_code', 'year'], inplace=True)\n",
    "\n",
    "# Generate a new variable 'StateYear' based on grouping by 'state_code' and 'year'\n",
    "merged_data['StateYear'] = merged_data.groupby(['state_code', 'year']).ngroup()\n",
    "\n",
    "# Save the DataFrame to a file\n",
    "merged_data.to_stata('DD_AnalysisDataset_NYR.dta', write_index=False, convert_dates={'Date': 'td'})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6666f3a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#The REGRESSION!\n",
    "\n",
    "#Step one is to first-difference everything on panelid\n",
    "\n",
    "# Mean-difference all variables on panelid\n",
    "merged_data['A_fips'] = merged_data['fips']\n",
    "merged_data['A_month'] = merged_data['month']\n",
    "merged_data['A_year'] = merged_data['year']\n",
    "merged_data['A_StateYear'] = merged_data['StateYear']\n",
    "merged_data['A_StateCode'] = merged_data['state_code']\n",
    "merged_data['A_EstTemp'] = merged_data['EstTempFlag']\n",
    "merged_data['A_EstTempPrcp'] = merged_data['EstTempFlagprcp']\n",
    "\n",
    "# Keep selected columns\n",
    "merged_data = merged_data.filter(regex='^lozone_|^treat|^Temp|^Rain|^Snow|^DOY|^Trend|^QTrend|^DateS|_D|_R|income|panelid|^A_')\n",
    "\n",
    "# Sort by panelid\n",
    "merged_data.sort_values(by='panelid', inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90439f0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# List to store generated columns\n",
    "generated_columns = []\n",
    "\n",
    "# Iterate over selected columns\n",
    "for var in merged_data.filter(regex='^lozone_|^treat|^Temp|^Rain|^Snow|^DOY|^Trend|^QTrend|^DateS|_D|_R|income').columns:\n",
    "    # Calculate mean by panelid\n",
    "    merged_data[f'M{var}'] = merged_data.groupby('panelid')[var].transform('mean')\n",
    "    # Generate differenced variable\n",
    "    generated_columns.append(merged_data[var] - merged_data[f'M{var}'])\n",
    "\n",
    "# Concatenate all generated columns at once\n",
    "merged_data = pd.concat([merged_data] + generated_columns, axis=1)\n",
    "\n",
    "# Drop original and mean columns\n",
    "merged_data.drop(columns=merged_data.filter(like='M').columns, inplace=True)\n",
    "\n",
    "# Save to a new file\n",
    "merged_data.to_stata('DD_AnalysisDataset_Diffed_NYR.dta', write_index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a133a08d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wait now actually the regressions!\n",
    "\n",
    "import statsmodels.api as sm\n",
    "\n",
    "# Define independent and dependent variables\n",
    "X = merged_data.filter(regex='^treat').copy()  # Independent variables\n",
    "y = merged_data['lozone_maxD']  # Dependent variable\n",
    "\n",
    "# Add constant term if needed\n",
    "# X = sm.add_constant(X)\n",
    "\n",
    "# Perform regression with clustered standard errors\n",
    "model = sm.OLS(y, X).fit(cov_type='cluster', cov_kwds={'groups': merged_data['A_StateYear']})\n",
    "\n",
    "# Print regression summary\n",
    "print(model.summary())\n",
    "\n",
    "# Test parameter equality\n",
    "hypotheses = 'treat_rvpIID = treat_rfgD, treat_rvpIID = treat_CARBD, treat_rfgD = treat_CARBD'\n",
    "print(model.t_test(hypotheses).summary())\n",
    "\n",
    "# Output regression results to a text file\n",
    "with open('DDResults_NYR.txt', 'w') as f:\n",
    "    f.write(model.summary().as_text())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1843de0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define independent and dependent variables\n",
    "X = merged_data.filter(regex='^treat|_RY').copy()  # Independent variables\n",
    "y = merged_data['lozone_maxD']  # Dependent variable\n",
    "\n",
    "# Perform regression with clustered standard errors\n",
    "model = sm.OLS(y, X).fit(cov_type='cluster', cov_kwds={'groups': merged_data['A_StateYear']})\n",
    "\n",
    "# Print regression summary\n",
    "with open('DDResults_NYR.txt', 'a') as f:\n",
    "    f.write(model.summary().as_text() + '\\n\\n')  # Append results to text file\n",
    "\n",
    "# Test parameter equality\n",
    "hypotheses = 'treat_rvpIID = treat_rfgD, treat_rvpIID = treat_CARBD, treat_rfgD = treat_CARBD'\n",
    "with open('DDResults_NYR.txt', 'a') as f:\n",
    "    f.write(model.t_test(hypotheses).summary().as_text() + '\\n\\n')  # Append results to text file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b80a1a72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Regression 1: Adding DOY and DOW controls\n",
    "X1 = merged_data.filter(regex='^treat|_R').copy()  # Independent variables\n",
    "y1 = merged_data['lozone_maxD']  # Dependent variable\n",
    "\n",
    "model1 = sm.OLS(y1, X1).fit(cov_type='cluster', cov_kwds={'groups': merged_data['A_StateYear']})\n",
    "\n",
    "# Test parameter equality\n",
    "hypotheses1 = 'treat_rvpIID = treat_rfgD, treat_rvpIID = treat_CARBD, treat_rfgD = treat_CARBD'\n",
    "with open('DDResults_NYR.txt', 'a') as f:\n",
    "    f.write(model1.t_test(hypotheses1).summary().as_text() + '\\n\\n')  # Append results to text file\n",
    "\n",
    "\n",
    "# Regression 2: Adding weather and weather cross effects\n",
    "X2 = merged_data.filter(regex='^treat|Temp|Rain|Snow|DOY|_D|_R').copy()  # Independent variables\n",
    "y2 = merged_data['lozone_maxD']  # Dependent variable\n",
    "\n",
    "model2 = sm.OLS(y2, X2).fit(cov_type='cluster', cov_kwds={'groups': merged_data['A_StateYear']})\n",
    "\n",
    "# Test parameter equality\n",
    "hypotheses2 = 'treat_rvpIID = treat_rfgD, treat_rvpIID = treat_CARBD, treat_rfgD = treat_CARBD'\n",
    "with open('DDResults_NYR.txt', 'a') as f:\n",
    "    f.write(model2.t_test(hypotheses2).summary().as_text() + '\\n\\n')  # Append results to text file\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d63726b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Regression 1: Adding income\n",
    "X1 = merged_data.filter(regex='^treat|income|Temp|Rain|Snow|DOY|_D|_R').copy()  # Independent variables\n",
    "y1 = merged_data['lozone_maxD']  # Dependent variable\n",
    "\n",
    "model1 = sm.OLS(y1, X1).fit(cov_type='cluster', cov_kwds={'groups': merged_data['A_StateYear']})\n",
    "\n",
    "# Test parameter equality\n",
    "hypotheses1 = 'treat_rvpIID = treat_rfgD, treat_rvpIID = treat_CARBD, treat_rfgD = treat_CARBD'\n",
    "with open('DDResults_NYR.txt', 'a') as f:\n",
    "    f.write(model1.t_test(hypotheses1).summary().as_text() + '\\n\\n')  # Append results to text file\n",
    "\n",
    "\n",
    "# Regression 2: Adding linear trends\n",
    "X2 = merged_data.filter(regex='^treat|income|Trend|Temp|Rain|Snow|DOY|_D|_R').copy()  # Independent variables\n",
    "y2 = merged_data['lozone_maxD']  # Dependent variable\n",
    "\n",
    "model2 = sm.OLS(y2, X2).fit(cov_type='cluster', cov_kwds={'groups': merged_data['A_StateYear']})\n",
    "\n",
    "# Test parameter equality\n",
    "hypotheses2 = 'treat_rvpIID = treat_rfgD, treat_rvpIID = treat_CARBD, treat_rfgD = treat_CARBD'\n",
    "with open('DDResults_NYR.txt', 'a') as f:\n",
    "    f.write(model2.t_test(hypotheses2).summary().as_text() + '\\n\\n')  # Append results to text file\n",
    "\n",
    "\n",
    "# Regression 3: Adding quadratic trends\n",
    "X3 = merged_data.filter(regex='^treat|income|Trend|QTrend|Temp|Rain|Snow|DOY|_D|_R').copy()  # Independent variables\n",
    "y3 = merged_data['lozone_maxD']  # Dependent variable\n",
    "\n",
    "model3 = sm.OLS(y3, X3).fit(cov_type='cluster', cov_kwds={'groups': merged_data['A_StateYear']})\n",
    "\n",
    "# Test parameter equality\n",
    "hypotheses3 = 'treat_rvpIID = treat_rfgD, treat_rvpIID = treat_CARBD, treat_rfgD = treat_CARBD'\n",
    "with open('DDResults_NYR.txt', 'a') as f:\n",
    "    f.write(model3.t_test(hypotheses3).summary().as_text() + '\\n\\n')  # Append results to text file\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2444f8ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Looking at the dependent variable epa_8hr\n",
    "\n",
    "# Regression 1: Base regression\n",
    "X1 = merged_data.filter(regex='^treat').copy()  # Independent variables\n",
    "y1 = merged_data['lozone_8hrD']  # Dependent variable\n",
    "\n",
    "model1 = sm.OLS(y1, X1).fit(cov_type='cluster', cov_kwds={'groups': merged_data['A_StateYear']})\n",
    "\n",
    "# Test parameter equality\n",
    "hypotheses1 = 'treat_rvpIID = treat_rfgD, treat_rvpIID = treat_CARBD, treat_rfgD = treat_CARBD'\n",
    "with open('DDResults_NYR.txt', 'a') as f:\n",
    "    f.write(model1.t_test(hypotheses1).summary().as_text() + '\\n\\n')  # Append results to text file\n",
    "\n",
    "\n",
    "# Regression 2: Adding year * region FE\n",
    "X2 = merged_data.filter(regex='^treat|_RY').copy()  # Independent variables\n",
    "y2 = merged_data['lozone_8hrD']  # Dependent variable\n",
    "\n",
    "model2 = sm.OLS(y2, X2).fit(cov_type='cluster', cov_kwds={'groups': merged_data['A_StateYear']})\n",
    "\n",
    "# Test parameter equality\n",
    "with open('DDResults_NYR.txt', 'a') as f:\n",
    "    f.write(model2.t_test(hypotheses1).summary().as_text() + '\\n\\n')  # Append results to text file\n",
    "\n",
    "\n",
    "# Regression 3: Adding DOY and DOW controls\n",
    "X3 = merged_data.filter(regex='^treat|_R').copy()  # Independent variables\n",
    "y3 = merged_data['lozone_8hrD']  # Dependent variable\n",
    "\n",
    "model3 = sm.OLS(y3, X3).fit(cov_type='cluster', cov_kwds={'groups': merged_data['A_StateYear']})\n",
    "\n",
    "# Test parameter equality\n",
    "with open('DDResults_NYR.txt', 'a') as f:\n",
    "    f.write(model3.t_test(hypotheses1).summary().as_text() + '\\n\\n')  # Append results to text file\n",
    "\n",
    "\n",
    "# Regression 4: Adding weather and weather cross effects\n",
    "X4 = merged_data.filter(regex='^treat|Temp|Rain|Snow|DOY|_D|_R').copy()  # Independent variables\n",
    "y4 = merged_data['lozone_8hrD']  # Dependent variable\n",
    "\n",
    "model4 = sm.OLS(y4, X4).fit(cov_type='cluster', cov_kwds={'groups': merged_data['A_StateYear']})\n",
    "\n",
    "# Test parameter equality\n",
    "with open('DDResults_NYR.txt', 'a') as f:\n",
    "    f.write(model4.t_test(hypotheses1).summary().as_text() + '\\n\\n')  # Append results to text file\n",
    "\n",
    "\n",
    "# Regression 5: Adding income\n",
    "X5 = merged_data.filter(regex='^treat|income|Temp|Rain|Snow|DOY|_D|_R').copy()  # Independent variables\n",
    "y5 = merged_data['lozone_8hrD']  # Dependent variable\n",
    "\n",
    "model5 = sm.OLS(y5, X5).fit(cov_type='cluster', cov_kwds={'groups': merged_data['A_StateYear']})\n",
    "\n",
    "# Test parameter equality\n",
    "with open('DDResults_NYR.txt', 'a') as f:\n",
    "    f.write(model5.t_test(hypotheses1).summary().as_text() + '\\n\\n')  # Append results to text file\n",
    "\n",
    "\n",
    "# Regression 6: Adding linear trends\n",
    "X6 = merged_data.filter(regex='^treat|income|Trend|Temp|Rain|Snow|DOY|_D|_R').copy()  # Independent variables\n",
    "y6 = merged_data['lozone_8hrD']  # Dependent variable\n",
    "\n",
    "model6 = sm.OLS(y6, X6).fit(cov_type='cluster', cov_kwds={'groups': merged_data['A_StateYear']})\n",
    "\n",
    "# Test parameter equality\n",
    "with open('DDResults_NYR.txt', 'a') as f:\n",
    "    f.write(model6.t_test(hypotheses1).summary().as_text() + '\\n\\n')  # Append results to text file\n",
    "\n",
    "\n",
    "# Regression 7: Adding quadratic trends\n",
    "X7 = merged_data.filter(regex='^treat|income|Trend|QTrend|Temp|Rain|Snow|DOY|_D|_R').copy()  # Independent variables\n",
    "y7 = merged_data['lozone_8hrD']  # Dependent variable\n",
    "\n",
    "model7 = sm.OLS(y7, X7).fit(cov_type='cluster', cov_kwds={'groups': merged_data['A_StateYear']})\n",
    "\n",
    "# Test parameter equality\n",
    "with open('DDResults_NYR.txt', 'a') as f:\n",
    "    f.write(model7.t_test(hypotheses1).summary().as_text() + '\\n\\n')  # Append results to text file\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b653bc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load the dataset\n",
    "data = pd.read_stata('DD_AnalysisDataset_Diffed.dta')\n",
    "\n",
    "# Regress ozone on weather and seasonal/day effects\n",
    "# Assuming you've already performed this regression and obtained the residuals\n",
    "# The residuals are stored in the column 'O3_Resid'\n",
    "\n",
    "# Create a county type variable\n",
    "data['CountyType'] = 0\n",
    "data.loc[data['A_RVPCty'] == 1, 'CountyType'] = 1\n",
    "data.loc[(data['A_RFGCty'] == 1) | (data['A_RVPRFGCty'] == 1), 'CountyType'] = 2\n",
    "data.loc[(data['A_CARBCty'] == 1) | (data['A_CARBRFGCty'] == 1), 'CountyType'] = 3\n",
    "\n",
    "# Collapse for plotting\n",
    "plot_data = data.groupby(['CountyType', 'A_year'])['O3_Resid'].mean().reset_index()\n",
    "\n",
    "# Plot RVP vs Baseline\n",
    "plt.figure(figsize=(10, 6))\n",
    "for ctype in [1, 0]:\n",
    "    plt.plot(plot_data[plot_data['CountyType'] == ctype]['A_year'],\n",
    "             plot_data[plot_data['CountyType'] == ctype]['O3_Resid'],\n",
    "             label=f'County Type {ctype}')\n",
    "plt.title(\"Summer ozone concentrations, RVP vs. baseline counties\")\n",
    "plt.xlabel(\"Year\")\n",
    "plt.ylabel(\"Average daily maximum ozone, ppm\")\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.savefig('Graph_AnnualResids_RVP.png')\n",
    "plt.show()\n",
    "\n",
    "# Plot RFG vs Baseline\n",
    "plt.figure(figsize=(10, 6))\n",
    "for ctype in [2, 0]:\n",
    "    plt.plot(plot_data[plot_data['CountyType'] == ctype]['A_year'],\n",
    "             plot_data[plot_data['CountyType'] == ctype]['O3_Resid'],\n",
    "             label=f'County Type {ctype}')\n",
    "plt.title(\"Summer ozone concentrations, RFG vs. baseline counties\")\n",
    "plt.xlabel(\"Year\")\n",
    "plt.ylabel(\"Average daily maximum ozone, ppm\")\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.savefig('Graph_AnnualResids_RFG.png')\n",
    "plt.show()\n",
    "\n",
    "# Plot CARB vs Baseline\n",
    "plt.figure(figsize=(10, 6))\n",
    "for ctype in [3, 0]:\n",
    "    plt.plot(plot_data[plot_data['CountyType'] == ctype]['A_year'],\n",
    "             plot_data[plot_data['CountyType'] == ctype]['O3_Resid'],\n",
    "             label=f'County Type {ctype}')\n",
    "plt.title(\"Summer ozone concentrations, CARB vs. baseline counties\")\n",
    "plt.xlabel(\"Year\")\n",
    "plt.ylabel(\"Average daily maximum ozone, ppm\")\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.savefig('Graph_AnnualResids_CARB.png')\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
